---
title: Presenting regression results better with forest plots
author: Matt Russell
date: '2019-04-07'
slug: presenting-regression-results-better
categories:
  - Data visualization
tags:
  - Data viz
  - Regression
  - Communicating data
---

I was inspired by Sara Richter's presentation at the [2019 Conference on Statistical Practice](https://ww2.amstat.org/meetings/csp/2019/index.cfm) titled "Easy Ways to Make Data Visualizations More Effective". One of the themes I took away from Sara's presentation was that data visualization needs to be practiced, but good data visualization doesn't need to be difficult to be done well.

When presenting results from a regression, people often use tables. While this is good if we need the true value of something, tables may not be good if we seek to convey general trends in our results.

This is particularly true for output from regressions that contain multiple independent variables. In forestry applications, we may want to quickly understand the magnitude of these variables and which ones have the greatest impact on a variable of interest. For example, the diameter growth of a tree differs depending on various factors including the species, tree size, and the site conditions on which it is growing. 

As an example, [Anderson et al.](https://www.mdpi.com/1999-4907/9/12/747) fit a mixed-effects model that predicts the ten-year diameter growth of black spruce in Minnesota. They investigated five independent variables and their relationship with diameter growth, including the diameter at breast height (`DBH`), site index (`SI`), basal area in larger trees (`BAL`), basal area of the stand (`BA`), and the tree's crown position (a dummy variable with a 1 indicating the tree is a dominant or co-dominant tree). 

I've replicated their Table 4, which contains the standard output from a regression. This includes the regression coefficients and standard errors:

Parameter | Term | Coefficient | Std. Error
------------- | ------------- | ------------- | ------------- | 
$$B_{0}$$ | $${Intercept}$$ | -0.6041 | 0.2558
$$B_{1}$$ | $${DBH}$$ | 0.3673 | 0.0865
$$B_{2}$$ | $${DBH^2}$$ | -0.0012 | 0.0002
$$B_{3}$$ | $${log(SI-1.37)}$$ | 0.3611 | 0.0703
$$B_{4}$$ | $$\frac{BAL^2}{log(DBH+5)}$$ | -0.0011 | 0.0001
$$B_{5}$$ | $$\sqrt{BA}$$ | -0.1303 | 0.0225
$$B_{6}$$ | $${CrownPosition}$$ | 0.2047 | 0.0122

```{r,echo=F}
term<-c("Intercept","DBH","DBH-squared","SI","BAL","BA","CrownPos")
coef<-c(-0.6041,0.3673,-0.0012,0.3611,-0.0011,-0.1303,0.2047)
se<-c(0.2558,0.0865,0.0002,0.0703,0.0001,0.0225,0.0122)
reg<-data.frame(term=term,coef=coef,se=se)
reg$lowconf<-reg$coef-(1.96*reg$se)
reg$highconf<-reg$coef+(1.96*reg$se)
reg$sign<-ifelse(coef>=0,"Pos","Neg")
```

The regression output indicates there are several variables that increase diameter growth: `DBH`, `SI`, and `CrownPosition`. In contrast, there are several variables that decrease diameter growth: `DBH-squared`, `BAL`, `BA`, and the intercept.
 
At first glance of the table, it's difficult to see which variables increase or decrease diameter growth. Although the standard errors are provided in the table, it's unclear to determine how "significant" each of the variables are, i.e., whether or not a confidence interval will contain zero.

A [forest plot (or blobbogram)](https://en.wikipedia.org/wiki/Forest_plot) can be used for information that shares a similar attribute. In our case, this is the coefficient for each of the regression parameters. Other applications include using them for [odds ratios](https://cran.r-project.org/web/packages/sjPlot/vignettes/plot_model_estimates.html) in logistic regression.

We can use a forest plot to visualize the results from the black spruce regression, with different colors indicating positive and negative coefficients and whiskers representing their 95% confidence limits:

```{r, echo=F,warning=F,message=F}
library(tidyverse)
#coef_table<-table(reg$coef)
#coef_levels<-names(coef_table)[order(coef_table)]
#reg$coef2<-factor(reg$coef,levels=coef_levels)
reg$term2<-factor(reg$term,
                  levels=c("Intercept","BA","BAL","DBH-squared","CrownPos","SI","DBH"))
p.forest<-ggplot(reg,aes(coef,term2,color=sign))+geom_point(size=4)+
  xlab("Coefficient")+
  ylab("")+
  geom_vline(xintercept=0,col="gray")+
  geom_errorbarh(aes(xmin=lowconf, xmax=highconf, height=0.01))+
  scale_color_manual(breaks = c("Pos", "Neg"),
                        values=c("red", "darkgreen"))+
  theme(axis.line = element_line(color="black"),
        axis.ticks = element_line(size = 0.5,color="black"),
        panel.background = element_rect(fill = "NA"),
        axis.text = element_text(color="black"),
        legend.position = "none")
p.forest
```

We often start our regression modeling with variables that don't end up being significant to the model. We can also show those in a forest plot, indicating to the reader that we investigated them, but they did not contribute to the model:

```{r,echo=F}
term<-c("Intercept","DBH","DBH-squared","SI","BAL","BA","CrownPos","DBH-cubed","BA-squared")
coef<-c(-0.6041,0.3673,-0.0012,0.3611,-0.0011,-0.1303,0.2047,0.1,-0.05)
se<-c(0.2558,0.0865,0.0002,0.0703,0.0001,0.0225,0.0122,0.09,0.05)
reg2<-data.frame(term=term,coef=coef,se=se)
reg2$lowconf<-reg2$coef-(1.96*reg2$se)
reg2$highconf<-reg2$coef+(1.96*reg2$se)
reg2$sign<-ifelse(coef>=0.11,"Pos",ifelse(reg2$term %in% c("BA-squared","DBH-cubed"),"Zero","Neg"))

reg2$term2<-factor(reg2$term,
                  levels=c("Intercept","BA","BAL","DBH-squared","BA-squared","DBH-cubed","CrownPos","SI","DBH"))
p.forest2<-ggplot(reg2,aes(coef,term2,color=sign))+geom_point(size=4)+
  xlab("Coefficient")+
  ylab("")+
  geom_vline(xintercept=0,col="gray")+
  geom_errorbarh(aes(xmin=lowconf, xmax=highconf, height=0.01))+
   scale_color_manual(breaks = c("Zero", "Pos", "Neg"),
                        values=c("red", "darkgreen", "gray"))+
  theme(axis.line = element_line(color="black"),
        axis.ticks = element_line(size = 0.5,color="black"),
        panel.background = element_rect(fill = "NA"),
        axis.text = element_text(color="black"),
        legend.position = "none")
p.forest2
```

In this case, the variables `DBH-cubed` and `BA-squared` (not very biologically-important ones) were included in the model, but were not significant. This is indicated by the 95% confidence intervals that include zero (which can be deemphasized with a gray color).  

Forest plots can also be used in regressions to [compare different models](https://cran.r-project.org/web/packages/jtools/vignettes/summ.html) and are always useful when comparing different values along a common scale.

*By Matt Russell. Leave a comment below or [email Matt](mailto:matt@arbor-analytics.com) with any questions or comments.*
