library(ggplot2)
diagPlot <- function(model){
#Residuals
p1<-ggplot(model, aes(.fitted, .resid))+geom_point()
p1<-p1+stat_smooth(method="loess", se = FALSE, na.rm = TRUE)
p1<-p1+geom_hline(yintercept=0, col="red", linetype="dashed")
p1<-p1+xlab("Fitted values")+ylab("Residuals")
p1<-p1+ggtitle("Residual vs Fitted Plot")+theme_bw()
#Standardized residuals
p2<-ggplot(model, aes(.fitted, .stdresid))+geom_point(na.rm=TRUE)
p2<-p2+geom_hline(yintercept=0, col="red", linetype="dashed")+
geom_hline(yintercept=2, col="black", linetype="dashed")+
geom_hline(yintercept=-2, col="black", linetype="dashed")
p2<-p2+ylab("Standardized residuals")+xlab("Fitted Value")
p2<-p2+ggtitle("Standardized residuals versus Fitted Plot")+theme_bw()
#Square root of standardized residuals
p3<-ggplot(model, aes(.fitted, sqrt(abs(.stdresid))))+geom_point(na.rm=TRUE)
p3<-p3+stat_smooth(method="loess", se = FALSE, na.rm = TRUE)+xlab("Fitted Value")
p3<-p3+ylab(expression(sqrt("|Standardized residuals|")))
p3<-p3+ggtitle("Scale-Location")+theme_bw()
#Q-Q plot
p4<-ggplot(model, aes(qqnorm(.stdresid)[[1]], .stdresid))+geom_point(na.rm = TRUE)
p4<-p4+xlab("Theoretical Quantiles")+ylab("Standardized Residuals")
p4<-p4+ggtitle("Normal Q-Q")+theme_bw()
#Cook's distance
p5<-ggplot(model, aes(seq_along(.cooksd), .cooksd))+geom_bar(stat="identity", position="identity")
p5<-p5+xlab("Obs. Number")+ylab("Cook's distance")
p5<-p5+ggtitle("Cook's distance")+theme_bw()
return(list(p.res=p1, p.stdres=p2, p.sqrtres=p3, p.qq=p4, p.cooks=p5))
}
# Chunk 6
p.diag <- diagPlot(perch.lm1)
p.diag$p.res
# This is a classic case that we have violated the linear relationship.
# 1 POINT
# Chunk 1
library(tidyverse)
fish <- read_csv('C:\\Users\\russellm\\Documents\\Classes\\UMN\\NR 5021\\data\\fish.csv')
# 1 POINT
# Chunk 2
pairs(~Weight+Length1+Length2+Length3+Height+Width, data=fish)
# All three length variables appear to have a nearly 1:1 relationship (likely very high correlation). The data are clustered because of the seven different species analyzed.
# 1 POINT
# Chunk 3
perch <- fish %>%
filter(Species=="Perch")
# 1 POINT
# Chunk 4
perch.lm1 <- lm(Weight ~ Length3, data = perch)
summary(perch.lm1)
-707.923 + (36.629 * 30)
# A perch that has a Length3 of 30 cm would weigh 390.9 grams.
# 1 POINT
# Chunk 5
library(ggplot2)
diagPlot <- function(model){
#Residuals
p1<-ggplot(model, aes(.fitted, .resid))+geom_point()
p1<-p1+stat_smooth(method="loess", se = FALSE, na.rm = TRUE)
p1<-p1+geom_hline(yintercept=0, col="red", linetype="dashed")
p1<-p1+xlab("Fitted values")+ylab("Residuals")
p1<-p1+ggtitle("Residual vs Fitted Plot")+theme_bw()
#Standardized residuals
p2<-ggplot(model, aes(.fitted, .stdresid))+geom_point(na.rm=TRUE)
p2<-p2+geom_hline(yintercept=0, col="red", linetype="dashed")+
geom_hline(yintercept=2, col="black", linetype="dashed")+
geom_hline(yintercept=-2, col="black", linetype="dashed")
p2<-p2+ylab("Standardized residuals")+xlab("Fitted Value")
p2<-p2+ggtitle("Standardized residuals versus Fitted Plot")+theme_bw()
#Square root of standardized residuals
p3<-ggplot(model, aes(.fitted, sqrt(abs(.stdresid))))+geom_point(na.rm=TRUE)
p3<-p3+stat_smooth(method="loess", se = FALSE, na.rm = TRUE)+xlab("Fitted Value")
p3<-p3+ylab(expression(sqrt("|Standardized residuals|")))
p3<-p3+ggtitle("Scale-Location")+theme_bw()
#Q-Q plot
p4<-ggplot(model, aes(qqnorm(.stdresid)[[1]], .stdresid))+geom_point(na.rm = TRUE)
p4<-p4+xlab("Theoretical Quantiles")+ylab("Standardized Residuals")
p4<-p4+ggtitle("Normal Q-Q")+theme_bw()
#Cook's distance
p5<-ggplot(model, aes(seq_along(.cooksd), .cooksd))+geom_bar(stat="identity", position="identity")
p5<-p5+xlab("Obs. Number")+ylab("Cook's distance")
p5<-p5+ggtitle("Cook's distance")+theme_bw()
return(list(p.res=p1, p.stdres=p2, p.sqrtres=p3, p.qq=p4, p.cooks=p5))
}
# Chunk 6
p.diag <- diagPlot(perch.lm1)
p.diag$p.res
# This is a classic case that we have violated the linear relationship.
# 1 POINT
# Chunk 7
perch <- perch %>%
mutate(Length3sq = Length3^2)
perch.lm2<-lm(Weight ~ Length3 + Length3sq, data = perch)
summary(perch.lm2)
141.9216+(-21.9104*30)+(0.9220*(30^2))
# A perch that has a Length3 of 30 cm would weigh 314.4 grams.
# 1 POINT
# Chunk 8
p.diag <- diagPlot(perch.lm2)
p.diag$p.res
# The assumption of linear relationship appears to hold for this model. The trend line is closer to zero across all fitted values, but notice the megaphone pattern and mention transforming the data as a possibility.
# 1 POINT
# Chunk 9
perch.lm3<-lm(log(Weight) ~ Length3 + Length3sq, data = perch)
summary(perch.lm3)
# 1 POINT
# Chunk 10
p.diag <- diagPlot(perch.lm3)
p.diag$p.res
# The assumption of linear relationship appears to hold for this model. Transforming the data makes a residual plot that is a "shotgun pattern" with mean values around zero.
# 1 POINT
# Chunk 11
elm <- read_csv('C:\\Users\\russellm\\Documents\\Classes\\UMN\\NR 5021\\data\\elm.csv')
# 1 POINT
# Chunk 12
p <- pairs(~HT + DIA + CROWN_HEIGHT + CROWN_DIAM_WIDE + UNCOMP_CROWN_RATIO + CROWN_CLASS_CD, data=elm)
p
# 1 POINT
# Chunk 13
elm <- elm %>%
mutate(CROWN_CLASS_dummy = ifelse(CROWN_CLASS_CD <= 2, 1, 0))
# 1 POINT
# Chunk 14
#install.packages("leaps")
library(leaps)
elm.leaps <- regsubsets(HT ~ DIA + CROWN_HEIGHT + CROWN_DIAM_WIDE +
UNCOMP_CROWN_RATIO + factor(CROWN_CLASS_dummy),
data=elm,nbest=20)
# 1 POINT
# Chunk 15
plot(elm.leaps, scale = "adjr2")
# The highest adjusted R-squared is 0.92. Crown height, widest crown diameter, and uncompacted crown ratio are the three variables chosen in the final model.
# 1 POINT
#install.packages("leaps")
library(leaps)
elm.leaps <- regsubsets(HT ~ DIA + CROWN_HEIGHT + CROWN_DIAM_WIDE +
UNCOMP_CROWN_RATIO + factor(CROWN_CLASS_dummy),
data=elm,nbest=20)
# 1 POINT
elm <- elm %>%
mutate(CROWN_CLASS_dummy = ifelse(CROWN_CLASS_CD <= 2, 1, 0))
# 1 POINT
p <- pairs(~HT + DIA + CROWN_HEIGHT + CROWN_DIAM_WIDE + UNCOMP_CROWN_RATIO + CROWN_CLASS_CD, data=elm)
p
# 1 POINT
plot(elm.leaps, scale = "adjr2")
# The highest adjusted R-squared is 0.92. Crown height, widest crown diameter, and uncompacted crown ratio are the three variables chosen in the final model.
# 1 POINT
carbon_words <- carbon %>%
unnest_tokens(word, `Article Title`) %>%
count(`Year Group`, word, sort = TRUE)
total_words <- carbon_words %>%
group_by(`Year Group`) %>%
summarize(total = sum(n))
carbon_words <- left_join(carbon_words, total_words)
carbon_words
# Chunk 1
library(tidyverse)
library(readxl)
library(tidytext)
# Chunk 2
carbon <- read_excel("C://Users//russellm//Documents//Arbor//Data//forest_carbon_lit.xlsx")
years <- read_excel("C://Users//russellm//Documents//Arbor//Data//carbon_years.xlsx")
# Chunk 3
carbon <- carbon %>%
filter(!is.na(`Publication Year`) & `Publication Year` >= 2001)
carbon <- left_join(carbon, years)
carbon_year <- carbon %>%
group_by(`Publication Year`) %>%
summarize(num_papers = n()) %>%
arrange(desc(`Publication Year`))
ggplot(carbon_year, aes(x = `Publication Year`, y = num_papers)) +
geom_point() +
geom_line() +
labs(x = "Year of publication", y = "Number of forest carbon articles") +
theme(panel.background = element_rect(fill = "NA"),
axis.line = element_line(color="black"))
# Chunk 4
carbon_source <- carbon %>%
group_by(`Source Title`) %>%
summarize(num_papers = n()) %>%
arrange(desc(num_papers)) %>%
filter(num_papers > 40)
ggplot(carbon_source, aes(x = reorder(`Source Title`, num_papers), num_papers)) +
geom_bar(stat="identity") +
coord_flip() +
labs(x = "Source title", y = "Number of forest carbon articles") +
theme(panel.background = element_rect(fill = "NA"),
axis.line = element_line(color="black"))
carbon_words <- carbon %>%
unnest_tokens(word, `Article Title`) %>%
count(`Year Group`, word, sort = TRUE)
total_words <- carbon_words %>%
group_by(`Year Group`) %>%
summarize(total = sum(n))
carbon_words <- left_join(carbon_words, total_words)
carbon_words
ggplot(carbon_words, aes(n/total, fill = `Year Group`)) +
geom_histogram(show.legend = FALSE) +
#xlim(NA, 0.0009) +
facet_wrap(~`Year Group`, ncol = 6, scales = "free_y")
carbon_tf_idf <- carbon_words %>%
bind_tf_idf(word, `Year Group`, n)
carbon_tf_idf
carbon_tf_idf %>%
select(-total) %>%
arrange(desc(tf_idf))
library(forcats)
carbon_tf_idf %>%
#filter(`Year Group` == "2001-2005") %>%
group_by(`Year Group`) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = `Year Group`)) +
geom_col(show.legend = FALSE) +
facet_wrap(~`Year Group`, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
# Remove some "useless" words
carbon_tf_idf <- carbon_tf_idf %>%
filter(!word %in% c("semi", "vol", "2011", "does", "bay", "mikaelian", "preparing", "ter",
"in", "the", "for", "to", "effect", "use", "upon", "3", "1",
"issue", "least", "effective", "resulting", "eighteen", "o", "1992"))
c2<-carbon_tf_idf %>%
group_by(`Year Group`) %>%
slice_max(tf_idf, n = 10) %>%
ungroup()
carbon_tf_idf %>%
filter(n >=2) %>%
group_by(`Year Group`) %>%
slice_max(tf_idf, n = 10) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = `Year Group`)) +
geom_col(show.legend = FALSE) +
facet_wrap(~`Year Group`, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
blogdown:::serve_site()
carbon_tf_idf <- carbon_words %>%
bind_tf_idf(word, `Year Group`, n)
carbon_tf_idf %>%
select(-total) %>%
arrange(desc(tf_idf))
blogdown:::new_post_addin()
library(tidyverse)
# Chunk 1
## Load packages
library(tidyverse)
library(readxl)
library(openxlsx)
library(knitr)
library(kableExtra)
library(patchwork)
# Chunk 2
# Read in datasets
# All stands dataset
all <- read_excel("C://Users//russellm//Documents//Arbor//Projects//TTG//Data//ALL_Inventory_2019.xlsx", guess_max = 15000)
all <- all %>%
mutate(area_diff=`Gross Book Area` - `Gross Gis Area` )
p.area_diff <- ggplot(all, aes(x=`Gross Book Area`, y = area_diff)) +
ylab("Gross Book Area - Gross Gis Area (acres)") +
xlab("Gross Book Area (acres)") +
geom_point()
#p.area_diff
# Appalachian dataset
app <- read_excel("C://Users//russellm//Documents//Arbor//Projects//TTG//Data//Appalachian_Inventory_2019.xlsx", guess_max = 2000)
# Lake States dataset
ls <- read_excel("C://Users//russellm//Documents//Arbor//Projects//TTG//Data//LakeStates_Inventory_2019.xlsx")
# Northeast dataset
ne <- read_excel("C://Users//russellm//Documents//Arbor//Projects//TTG//Data//Northeast_Inventory_2019.xlsx")
# Pacific Northwest dataset
pnw <- read_excel("C://Users//russellm//Documents//Arbor//Projects//TTG//Data//PacificNorthwest_Inventory_2019.xlsx")
# Chunk 3
# Specific gravity datasets
sp_gr <- read_excel("C://Users//russellm//Documents//Arbor//Projects//TTG//Analysis//TTG_SPECIES.xlsx")
sp_gr_all <- read_excel("C://Users//russellm//Documents//Arbor//Projects//TTG//Analysis//TTG_SPECIES_ALL.xlsx")
# Rows in the regional datasets contain total volume across the tract.
#
# For pulpwood in ne dataset: Sum of pulpwood by species in ne dataset () = `Softwood TOP Total` + `Softwood PW Total` in ALL dataset (Same for Hardwood pulp)
#
# Pulpwood estimates include tops and limbs.
#
# Use Gross Book Area to calculate per-acre values.
## Northeast data
# Select pulp data and merge it with specific gravity
ne_pulp <- ne %>%
mutate(REGION = "NE") %>%
select(REGION, `Stand Key`, `Gross Book Area`, `Area Unit`, `Vol Units Pulp`,
`Fir Spruce Pulp`, `Hardwood Pulp`, `Hemlock Pulp`,
`Pine Pulp`, `Softwood Pulp`) %>%
pivot_longer(`Fir Spruce Pulp` : `Softwood Pulp`,
names_to = "SPECIES_PROD",
values_to = "PulpTons") %>%
inner_join(sp_gr, by = c("REGION", "SPECIES_PROD"))
# Convert to dry tons
ne_pulp <- ne_pulp %>%
mutate(PulpTonsDry = PulpTons * WOOD_SPGR_GREENVOL_DRYWT, # units are short tons, dry weight
PulpC_Tons = PulpTonsDry * 0.5, # units are carbon in short tons
Pulp_TonsCO2e = PulpC_Tons * 3.66667, # units are metric tonnes CO2 equivalent
PulpC_MT = PulpC_Tons * 0.9072, # units are carbon in metric tonnes
Pulp_MTCO2e = PulpC_MT * 3.66667) # units are metric tonnes CO2 equivalent
ne_pulp2<- ne_pulp %>%
group_by(`Stand Key`) %>%
summarize(sum_PulpC_Tons = sum(PulpC_Tons),
sum_Pulp_TonsCO2e = sum(Pulp_TonsCO2e),
sum_PulpC_MT = sum(PulpC_MT),
sum_Pulp_MTCO2e = sum(Pulp_MTCO2e))
# Select sawtimber data
# Select pulp data and merge it with specific gravity and tons/MBF conversion
ne_saw <- ne %>%
mutate(REGION = "NE") %>%
select(REGION, `Stand Key`, `Gross Book Area`, `Area Unit`, `Vol Units Solid`,
`Fir Spruce Saw`, `Hard Maple Saw`, `Hardwood Saw`, `Hemlock Saw`,
`Pine Saw`, `Red Oak Saw`, `Red Pine Saw`, `Soft Maple Saw`, `Softwood Saw`,
`White Pine Saw`,`Yellow Birch Saw`) %>%
pivot_longer(`Fir Spruce Saw` : `Yellow Birch Saw`,
names_to = "SPECIES_PROD",
values_to = "IntMBF") %>%
inner_join(sp_gr, by = c("REGION", "SPECIES_PROD"))
View(ne_saw)
mbf <- tibble(
~Stand, ~Species, ~`Volume (MBF)`,
1, "Eastern white pine", 6,
2, "Eastern white pine", 12,
3, "Eastern white pine", 18,
4, "Douglas-fir", 6,
5, "Douglas-fir", 12,
6, "Douglas-fir", 18,
7, "Southern pine", 6,
8, "Southern pine", 12,
9, "Southern pine", 18
)
library(tidyverse)
mbf <- tibble(
~Stand, ~Species, ~`Volume (MBF)`,
1, "Eastern white pine", 6,
2, "Eastern white pine", 12,
3, "Eastern white pine", 18,
4, "Douglas-fir", 6,
5, "Douglas-fir", 12,
6, "Douglas-fir", 18,
7, "Southern pine", 6,
8, "Southern pine", 12,
9, "Southern pine", 18
)
library(tidyverse)
mbf <- tribble(
~Stand, ~Species, ~`Volume (MBF)`,
1, "Eastern white pine", 6,
2, "Eastern white pine", 12,
3, "Eastern white pine", 18,
4, "Douglas-fir", 6,
5, "Douglas-fir", 12,
6, "Douglas-fir", 18,
7, "Southern pine", 6,
8, "Southern pine", 12,
9, "Southern pine", 18
)
View(mbf)
mbf %>%
kable("html", caption = 'Example forest inventory from nine stands.') %>%
kable_styling()
t_mbf <- tribble(
~Species, ~TonsMBF,
"Eastern white pine", 4.50,
"Douglas-fir", 4.35,
"Southern pine", 7.50,
)
mbf <- innerjoin(mbf, t_mbf)
mbf <- inner_join(mbf, t_mbf)
mbf
View(sp_gr)
sp_gr <- tribble(
~Species, ~WOOD_SPGR_GREENVOL_DRYWT,
"Eastern white pine", 0.34,
"Douglas-fir", 0.45,
"Southern pine", 0.47,
)
mbf <- inner_join(mbf, sp_gr)
mbf
# Chunk 1
library(forcats)
library(tidyverse)
library(knitr)
library(formattable)
library(kableExtra)
# Chunk 2
mbf <- tribble(
~Stand, ~Species, ~`Volume (MBF)`,
1, "Eastern white pine", 6,
2, "Eastern white pine", 12,
3, "Eastern white pine", 18,
4, "Douglas-fir", 6,
5, "Douglas-fir", 12,
6, "Douglas-fir", 18,
7, "Southern pine", 6,
8, "Southern pine", 12,
9, "Southern pine", 18
)
# Chunk 3
mbf %>%
kable("html", caption = 'Example forest inventory data from nine stands.') %>%
kable_styling()
# Chunk 4
t_mbf <- tribble(
~Species, ~TonsMBF,
"Eastern white pine", 4.50,
"Douglas-fir", 4.35,
"Southern pine", 7.50,
)
mbf <- inner_join(mbf, t_mbf)
# Chunk 5
sp_gr <- tribble(
~Species, ~WOOD_SPGR_GREENVOL_DRYWT, MerchTotConv,
"Eastern white pine", 0.34, 1.12,
"Douglas-fir", 0.45, 1.12,
"Southern pine", 0.47, 1.12
)
mbf <- inner_join(mbf, sp_gr)
# Chunk 1
library(forcats)
library(tidyverse)
library(knitr)
library(formattable)
library(kableExtra)
# Chunk 2
mbf <- tribble(
~Stand, ~Species, ~`Volume (MBF)`,
1, "Eastern white pine", 6,
2, "Eastern white pine", 12,
3, "Eastern white pine", 18,
4, "Douglas-fir", 6,
5, "Douglas-fir", 12,
6, "Douglas-fir", 18,
7, "Southern pine", 6,
8, "Southern pine", 12,
9, "Southern pine", 18
)
# Chunk 3
mbf %>%
kable("html", caption = 'Example forest inventory data from nine stands.') %>%
kable_styling()
# Chunk 4
t_mbf <- tribble(
~Species, ~TonsMBF,
"Eastern white pine", 4.50,
"Douglas-fir", 4.35,
"Southern pine", 7.50,
)
mbf <- inner_join(mbf, t_mbf)
sp_gr <- tribble(
~Species, ~WOOD_SPGR_GREENVOL_DRYWT, MerchTotConv,
"Eastern white pine", 0.34, 1.12,
"Douglas-fir", 0.45, 1.12,
"Southern pine", 0.47, 1.12
)
mbf <- inner_join(mbf, sp_gr)
sp_gr <- tribble(
~Species, ~WOOD_SPGR_GREENVOL_DRYWT, ~MerchTotConv,
"Eastern white pine", 0.34, 1.12,
"Douglas-fir", 0.45, 1.12,
"Southern pine", 0.47, 1.12
)
mbf <- inner_join(mbf, sp_gr)
mbf <- mbf %>%
mutate(`Biomass (Tons)` = `Volume (MBF)` * TonsMBF * WOOD_SPGR_GREENVOL_DRYWT * MerchTotConv,
`Carbon (Tons)` = `Biomass (Tons)` * 0.5)
mbf %>%
kable("html", caption = 'Carbon storage from nine stands.') %>%
kable_styling()
mbf <- mbf %>%
mutate(`Biomass (Tons)` = round(`Volume (MBF)` * TonsMBF * WOOD_SPGR_GREENVOL_DRYWT * MerchTotConv, 2),
`Carbon (Tons)` = round(`Biomass (Tons)` * 0.5, 2))
mbf %>%
kable("html", caption = 'Carbon storage from nine stands.') %>%
kable_styling()
blogdown:::serve_site()
blogdown:::serve_site()
# Chunk 1
library(tidyverse)
rice <- read_csv('C:\\Users\\russellm\\Documents\\Classes\\UMN\\NR 5021\\Fall 2020\\Labs\\Lab9\\wildrice.csv')
## 2 POINTS
# Chunk 2
rice_wetland <- rice %>%
group_by(WETTYPE) %>%
summarize(n = n())
rice_wetland
## Open water wetlands have the greatest number of lakes (n = 449).
## Shallow marshes have the fewest number of lakes (n = 21).
# 2 POINTS
# Chunk 3
p.rice <- ggplot(data = rice, aes(x = LAKE_ACRES, y = WILDRICE_ACRES, col = WETTYPE))+
geom_point()
p.rice
# 2 POINTS
# Chunk 4
cor(rice$LAKE_ACRES, rice$WILDRICE_ACRES)
## With a value of 0.33, the correlation is strongly weak to weakly moderate.
# 2 POINTS
# Chunk 5
rice_mean <- rice %>%
group_by(WETTYPE) %>%
summarize(mean_WILDRICE_ACRES = mean(WILDRICE_ACRES))
rice_mean
## Deep marshes have the greatest mean acres of wild rice (127 acres).
## Shallow marshes have the lowest mean acres of wild rice (60.2 acres). Open water wetlands are close (60.9 acres).
# 2 POINTS
# Chunk 6
rice.aov <- lm(WILDRICE_ACRES ~ WETTYPE, data = rice)
anova(rice.aov)
# Null hypothesis is that all wetland type means are equal. Alternative hypothesis is that at least one wetland type mean differs from the rest. The null hypothesis is rejected. because the p-value is 0.003914 , less than the level of significance of 0.05. The supervisor is correct.
# 10 POINTS
ric
rice
